{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pulling Price Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Intro here ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install requests\n",
    "!pip3 install pandas\n",
    "!pip3 install numpy\n",
    "!pip3 install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests as rq\n",
    "import pandas as pd\n",
    "import csv\n",
    "import orjson\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# We have to pull data in batches of 20 to avoid the API limit. The for loop is slow but it helps stay under the limit. \n",
    "\n",
    "for i in range(0, 1):\n",
    "    offset = i * 20 # Updates offset parameter \n",
    "    url = f\"https://gamma-api.polymarket.com/markets?offset={offset}\" # Updated offset\n",
    "    print(i)\n",
    "    \n",
    "    response = rq.get(url) # Gets data from the API\n",
    "    \n",
    "    # Check for okay\n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "        \n",
    "        # Collect the specific data we need\n",
    "        for entry in results:\n",
    "            data.append({\n",
    "                \"id\": entry.get(\"id\"),\n",
    "                \"question\": entry.get(\"question\"),\n",
    "                'clobTokenIds': entry.get(\"clobTokenIds\"),\n",
    "                \"createdAt\": entry.get(\"createdAt\"),\n",
    "                \"closedTime\": entry.get(\"closedTime\"),\n",
    "                \"volume\": entry.get('volume')})\n",
    "    else:\n",
    "        print(f\"Failed to retrieve data at offset {offset}\") #error message for failed loop\n",
    "        break  # break to end loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "good_data_save = data\n",
    "good_data_save"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Examplar output for the code above:\n",
    "\n",
    "'question': 'Will Joe Biden get Coronavirus before the election?',\n",
    "  'clobTokenIds': '[\"53135072462907880191400140706440867753044989936304433583131786753949599718775\", \"60869871469376321574904667328762911501870754872924453995477779862968218702336\"]',\n",
    "  'createdAt': '2020-10-02T16:10:01.467Z',\n",
    "  'closedTime': '2020-11-02 16:31:01+00'},\n",
    " {'id': '17',\n",
    "  'question': 'Will Airbnb begin publicly trading before Jan 1, 2021?',\n",
    "  'clobTokenIds': '[\"23957885615115430922384185661294483989521212430808224513177413172438775950057\", \"44065917169138815451032058926556960033374557137879250075091545322436931840853\"]',\n",
    "  'createdAt': '2020-10-02T19:20:04.234Z',\n",
    "  'closedTime': '2020-12-11 20:53:24+00'},"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_good = pd.DataFrame(good_data_save)\n",
    "print(data_df_good.columns)\n",
    "len(data_df_good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output for the code above was:\n",
    "\n",
    "Index(['id', 'question', 'clobTokenIds', 'createdAt', 'closedTime'], dtype='object')\n",
    "\n",
    "21075"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_good.to_csv('new_polydata.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df_good = pd.read_csv(r\"C:\\Users\\oogim\\Downloads\\new_polydata.csv\")\n",
    "bool_list = ~data_df_good['clobTokenIds'].isna() #filtering out Nan CLOB tokens\n",
    "data_df_best = data_df_good.loc[bool_list].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two CLOB tokens per market, each associated with the token for one outcome. For exampe, a token for \"yes\" and \"no\" on \"Will X Happen?\". By saving the data as a CSV, it turned the tuple into a different datatype that was unseperated, so we had to convert it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        (531350724629078801914001407064408677530449899...\n",
      "1        (239578856151154309223841856612944839895212124...\n",
      "2        (719297353092590301528700481322625696108672726...\n",
      "3        (448546879784327080536190768858503310506013419...\n",
      "4        (362723741995362299664450596440940506732699213...\n",
      "                               ...                        \n",
      "20999    (537855030533494087400152786648172286927910530...\n",
      "21000    (928620714814449982069306488872738618597308474...\n",
      "21001    (532598916201761901898544512095113341996657747...\n",
      "21002    (265278293478220398941737479227880899891131149...\n",
      "21003    (586682104015879392890207650005438062752241079...\n",
      "Name: clobTokenIds, Length: 21004, dtype: object\n"
     ]
    }
   ],
   "source": [
    "cleaned_CLOBs = []\n",
    "\n",
    "# The CLOBs were stored like this \"('num', 'num')\", but the numbers were not uniform in length, so to isolate num we found the location of the quotation marks\n",
    "\n",
    "for i in range(len(data_df_best)): \n",
    "    row = str(data_df_best.loc[i]['clobTokenIds']) # Turning it into a string\n",
    "    first_index = row.index('\"') # Finding the location of quotation marks\n",
    "    second_index = row.index('\"', first_index+1)\n",
    "    third_index = row.index('\"', second_index+1)\n",
    "    fourth_index = row.index('\"', third_index+1)\n",
    "\n",
    "    #Now we have the locations of the quotation marks, we can isolate the IDs in between. \n",
    "    cleaned_CLOBs.append((int(data_df_best.loc[i]['clobTokenIds'][first_index+1:second_index]), int(data_df_best.loc[i]['clobTokenIds'][third_index+1:fourth_index])))\n",
    "    #appending new int CLOBs to list\n",
    "\n",
    "len(cleaned_CLOBs) == len(data_df_best) #checking nothing was lost\n",
    "\n",
    "data_df_best['clobTokenIds']=cleaned_CLOBs #replacing old CLOBs with cleaned values\n",
    "print(data_df_best['clobTokenIds'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we had the CLOB token IDs, we were able to pull price data from Polymarket's API. However, after looking at the data, we realized price history didn't exist until market 4720 in our dataset. After that point, the data was filled with .5, sometimes interspersed with actual price data, we're not sure why. The long chains of .5 disappeared at around market 8479 in our dataset. We discovered this through trial and error with the following block of code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38533708059183496482396436337628663016977781241069578366354949231739219895465\n",
      "https://clob.polymarket.com/prices-history?market=38533708059183496482396436337628663016977781241069578366354949231739219895465&startTs=0&endTs=10000000000\n",
      "200\n",
      "https://clob.polymarket.com/prices-history?market=7383078199576823886643038741729069867468150381224481526293786173706803650616&startTs=0&endTs=10000000000\n",
      "200\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "import requests as rq\n",
    "\n",
    "a, b = data_df_best['clobTokenIds'][4900] #Figuring out GOOD data started at 8479\n",
    "print(a)\n",
    "url_a = f\"https://clob.polymarket.com/prices-history?market={a}&startTs=0&endTs=10000000000\"# Updated offset\n",
    "print(url_a)\n",
    "response_a = rq.get(url_a) # Gets data from the API\n",
    "print(response_a.status_code)\n",
    "results_a = response_a.json()\n",
    "url_b = f\"https://clob.polymarket.com/prices-history?market={b}&startTs=0&endTs=10000000000\"# Updated offset\n",
    "print(url_b)\n",
    "response_b = rq.get(url_b) # Gets data from the API\n",
    "print(response_b.status_code)\n",
    "results_b = response_b.json()\n",
    "print(results_a['history'][100]['p'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following block of code is how we pulled the price data for each token. We batched the data into sets of 250 markets, as it was too large to keep it all in one file. The price data was stored as midpoints, so we only needed to pull the price history for one token ID per pair, as they were equal to 1 minus the other. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data = []\n",
    "\n",
    "# Offset Loop\n",
    "\n",
    "startpoint = 8479\n",
    "loopcount = 1\n",
    "chunk_size = 250\n",
    "\n",
    "length = len(data_df_best) - startpoint # Calculating the number of markets we will have data for\n",
    "chunk_total = int(math.ceil(length/chunk_size)) # The total number of files we will have\n",
    "print(f'You shall receieve {chunk_total} files, get excited!')\n",
    "\n",
    "while loopcount <= chunk_total: # Iterating until we reach the number of files we expect\n",
    "    # The loop inside a loop is slow but it avoided getting rate limited, which was breaking our loop. It avoided us having to add a wait timer between calls.\n",
    "    for i in list(range(startpoint + (chunk_size * loopcount), startpoint+(chunk_size *(loopcount+1)))):\n",
    "        if i > len(data_df_best)-1:\n",
    "                break\n",
    "        a, b = data_df_best['clobTokenIds'][i]\n",
    "        url_a = f\"https://clob.polymarket.com/prices-history?market={a}&startTs=0&endTs=10000000000\" # Updated offset\n",
    "        response_a = rq.get(url_a) # Gets data from the API\n",
    "        \n",
    "        if response_a.status_code == 200: # Checks the call was successful\n",
    "            results_a = response_a.json()\n",
    "            print(f'Success at entry {i}!')\n",
    "            \n",
    "            # Collecting the specific data we need\n",
    "            price_data.append({\n",
    "                'identification': int(data_df_best['id'][i]),\n",
    "                'CLOB_a': results_a['history']})\n",
    "            \n",
    "        # If the call fails, break\n",
    "        else:\n",
    "            print(f\"Failed to retrieve data at {i}\") \n",
    "            break\n",
    "    \n",
    "    # If we're not at the final chunk, title the file start point to start point + chunk size\n",
    "    if loopcount != chunk_total: \n",
    "        filename = f'price_data_from_{startpoint + (chunk_size * loopcount)}_to_{startpoint + (chunk_size * (loopcount + 1)) - 1}.json'\n",
    "\n",
    "    # If we're at the final chunk, title the file start point to start point + expected length of dataset\n",
    "    else: \n",
    "         filename = f'price_data_from_{startpoint + (chunk_size * loopcount)}_to_{len(data_df_best)-1}.json'\n",
    "\n",
    "    # Saving the file with orjson as it was ~6 times faster than json\n",
    "    with open(filename, 'wb') as json_file:\n",
    "        json_file.write(orjson.dumps(price_data, option=orjson.OPT_INDENT_2))\n",
    "    print(f\"Saved: {filename}\")\n",
    "    \n",
    "    price_data.clear()   \n",
    "    \n",
    "    loopcount += 1\n",
    "\n",
    "print(\"Data collection complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output for the code above was: \n",
    "\n",
    "You shall receieve 50 files, get excited!\n",
    "Success at entry 20979!\n",
    "Success at entry 20980!\n",
    "Success at entry 20981!\n",
    "Success at entry 20982!\n",
    "...\n",
    "Success at entry 21002!\n",
    "Success at entry 21003!\n",
    "Saved: price_data_from_20979_to_21003.json\n",
    "Data collection complete"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
